1
00:00:01,339 --> 00:00:03,580
So now, we're going to do the same tasks

2
00:00:03,580 --> 00:00:07,339
using Python. We're done with Scala, you can open up a

3
00:00:07,339 --> 00:00:11,559
new terminal or close the other one, it's up to you. Open up a new terminal

4
00:00:11,559 --> 00:00:15,940
and start up the pyspark shell
located under the spark_home/

5
00:00:15,940 --> 00:00:22,940
/bin directory.

6
00:00:26,650 --> 00:00:30,270
Now, you are going to create the RDD of the log file

7
00:00:30,270 --> 00:00:34,330
from the first exercise. The log file 

8
00:00:34,330 --> 00:00:41,330
was called sparkLog.out, so go ahead
and create the RDD for that.

9
00:00:47,100 --> 00:00:49,780
Next filter

10
00:00:49,780 --> 00:00:52,699
out the lines that contains the keyword
'info'

11
00:00:52,699 --> 00:00:59,699
from the log file.

12
00:01:04,400 --> 00:01:08,580
And then get the count of the number lines with the word info in it.

13
00:01:08,580 --> 00:01:13,800
Remember, your results will be different
from what I have as we probably loaded

14
00:01:13,800 --> 00:01:14,720
different log files.

15
00:01:14,720 --> 00:01:19,330
You can always chain

16
00:01:19,330 --> 00:01:23,390
operations together so here we filter
and do a count

17
00:01:23,390 --> 00:01:26,680
to find a number of

18
00:01:26,680 --> 00:01:32,430
Spark in the lines. You can also do a
collect

19
00:01:32,430 --> 00:01:36,100
to have it print out the lines

20
00:01:36,100 --> 00:01:43,100
that fits the filters criteria.

21
00:01:43,750 --> 00:01:47,010
And finally you can also view the DAG

22
00:01:47,010 --> 00:01:50,850
of an RDD using the toDebugString().

23
00:01:56,500 --> 00:02:00,020
Now we are going to get into joining RDDs 

24
00:02:00,020 --> 00:02:04,640
in Python. As you have done before

25
00:02:04,640 --> 00:02:08,470
it's going to be the same you have to
create the two RDD's for the readme

26
00:02:08,470 --> 00:02:09,030
file

27
00:02:09,030 --> 00:02:16,030
and the changes.txt file.

28
00:02:21,650 --> 00:02:24,840
Let's find out how many spark keywords are in each file

29
00:02:24,840 --> 00:02:31,840
but filtering out and then doing a count
on it.

30
00:02:37,890 --> 00:02:44,890
Do the same thing for the changes file.

31
00:02:47,190 --> 00:02:51,220
Let's do a word count to get the number
words and their occurrences

32
00:02:51,220 --> 00:02:55,760
in each of the RDD. First,

33
00:02:55,760 --> 00:03:00,420
you do the flattmap operation to break
up the words into their own line

34
00:03:00,420 --> 00:03:03,580
and then you map the words to the value
1

35
00:03:03,580 --> 00:03:09,370
and then you reduce by that key to add up all the number up occurrences of a

36
00:03:09,370 --> 00:03:16,370
particular word.

37
00:03:18,409 --> 00:03:25,409
Let's do the same for the changes.txt file.

38
00:03:34,900 --> 00:03:39,330
likewise you can do the collect
operation on it to see the

39
00:03:39,330 --> 00:03:46,330
actual word count of each of the files.

40
00:03:50,770 --> 00:03:52,450
So now let's just join

41
00:03:52,450 --> 00:03:56,530
these two datasets together to produce a collective word count

42
00:03:56,530 --> 00:04:00,910
from both files. So if the words are the same, they will be counted and added together.

43
00:04:00,910 --> 00:04:04,800
We are going to use the 

44
00:04:04,800 --> 00:04:08,550
join operation. So create a

45
00:04:08,550 --> 00:04:13,290
joined RDD and then the first RRD.join

46
00:04:13,290 --> 00:04:20,280
to the 2nd RDD. Let's go ahead and cache

47
00:04:20,280 --> 00:04:25,729
this RDD and the print

48
00:04:25,729 --> 00:04:30,830
the value out to the console. So each key

49
00:04:30,830 --> 00:04:34,430
is followed by

50
00:04:34,430 --> 00:04:37,620
an array of the values for that particular key

51
00:04:37,620 --> 00:04:44,620
or the word.

52
00:04:46,480 --> 00:04:52,200
Now just like with Scala, we are going to combine it. So we're going to run an operation to

53
00:04:52,200 --> 00:04:56,070
add the sums to the values together for a
particular key, so instead of seeing

54
00:04:56,070 --> 00:04:59,720
a key with an array of values, you are going to see a

55
00:04:59,720 --> 00:05:03,560
key with the total sum of the value, so
that's what we just did

56
00:05:03,560 --> 00:05:10,560
to have a joinedSum. Now are going to do the take(5) to get the first five values

57
00:05:10,890 --> 00:05:12,120
from each set -- the old 

58
00:05:12,120 --> 00:05:15,170
set and the new set in order to compare

59
00:05:15,170 --> 00:05:18,350
the sums of these two. You see that they both

60
00:05:18,350 --> 00:05:22,720
are equal, indicating that the operation
completed successfully.

61
00:05:22,720 --> 00:05:31,360
Now we are going to get into shared variables. Pythons also has these two shared variables 

62
00:05:31,440 --> 00:05:34,950
available with Spark or
rather Spark has

63
00:05:34,950 --> 00:05:38,350
these shared variables available with Python. Broadcast variables

64
00:05:38,420 --> 00:05:43,250
is the first one. You created the same
way using the Spark Context.

65
00:05:43,250 --> 00:05:48,620
and these broadcast variables are useful for when the datasets gets really large

66
00:05:48,620 --> 00:05:53,310
so you don't have to send each of the dataset out to the worker nodes, 

67
00:05:53,310 --> 00:05:55,690
rather you just out the broadcast value

68
00:05:55,690 --> 00:05:59,140
and then the worker nodes just invoke the .value() 

69
00:05:59,140 --> 00:06:02,610
operation to grab the values. Then,

70
00:06:02,610 --> 00:06:06,010
accumulators, on the other hand, can only be added

71
00:06:06,010 --> 00:06:09,570
through as associative operation by a worker node.

72
00:06:09,570 --> 00:06:13,190
Again creating an accumulator

73
00:06:13,190 --> 00:06:19,889
values starts at 0.

74
00:06:19,889 --> 00:06:24,050
and then we are going to test this out by creating an RDD 

75
00:06:24,050 --> 00:06:29,739
with the values one two three and four. Then we are going to define a function

76
00:06:29,739 --> 00:06:33,619
which will add in the values as a

77
00:06:33,619 --> 00:06:36,649
given array into

78
00:06:36,649 --> 00:06:41,159
the accumulator. Here I am defining the
accumulator

79
00:06:41,159 --> 00:06:45,919
and I'm going to add each value of the
element into the accumulator.

80
00:06:45,919 --> 00:06:52,349
Then iterate through

81
00:06:52,349 --> 00:06:56,439
each element of the original RDD, the one with the values one through four

82
00:06:56,439 --> 00:06:59,509
and pass in that function we just created.

83
00:06:59,509 --> 00:07:02,589
So the function will

84
00:07:02,589 --> 00:07:09,589
take each value of the array and add it to
the accumulator.

85
00:07:18,550 --> 00:07:22,300
and we need to call out the accum.value() to see the value of the

86
00:07:22,300 --> 00:07:23,060
accumulator.

87
00:07:23,060 --> 00:07:26,090
Remember, only be driver node

88
00:07:26,090 --> 00:07:29,700
can access the value of the accumulator.
Worker nodes can only

89
00:07:29,700 --> 00:07:33,530
add to it. They cannot see the value. Finally,

90
00:07:33,530 --> 00:07:39,430
again a quick part on the key/value pairs.
We have seen this already with

91
00:07:39,430 --> 00:07:42,830
the joining of the RDDs in Python

92
00:07:42,830 --> 00:07:47,690
but it is zero-indexed, so pair[0] will give you the first element and pair[1] will

93
00:07:47,690 --> 00:07:48,840
give you the second.

